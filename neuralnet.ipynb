{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from cnn import CNN, ComplexCNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load and preprocess dataset\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/train.csv/train.csv')\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "# Build vocab\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# Build vocab and include '<pad>' and '<unk>' tokens\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_df['comment_text']), specials=[\"<unk>\", \"<pad>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset since dataset is large\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for (_text, _label) in batch:\n",
    "         label_list.append(_label)\n",
    "         processed_text = torch.tensor(_text, dtype=torch.int64)\n",
    "         text_list.append(processed_text)\n",
    "         lengths.append(processed_text.size(0))\n",
    "    # Pad the sequence\n",
    "    text_list = pad_sequence(text_list, batch_first=True)\n",
    "    label_list = torch.stack(label_list)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    return text_list, label_list, lengths\n",
    "\n",
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, dataframe, text_field, label_fields, tokenizer, vocab):\n",
    "        self.dataframe = dataframe\n",
    "        self.text_field = text_field\n",
    "        self.label_fields = label_fields\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = [vocab[token] for token in tokenizer(self.dataframe.iloc[idx][self.text_field])]\n",
    "        # Convert label columns to a consistent numeric type (e.g., float)\n",
    "        labels = self.dataframe.iloc[idx][self.label_fields].astype(float).values\n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(labels, dtype=torch.float)\n",
    "\n",
    "\n",
    "# Create instances of the CommentDataset\n",
    "train_dataset = CommentDataset(train_df, 'comment_text', ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], tokenizer, vocab)\n",
    "test_dataset = CommentDataset(test_df, 'comment_text', ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], tokenizer, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders init\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "### define CNN model\n",
    "\n",
    "INPUT_DIM = len(vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [2, 3, 4]\n",
    "OUTPUT_DIM = len(train_dataset.label_fields)  # Number of labels\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)\n",
    "# model = ComplexCNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CNN' object has no attribute 'embedded_to_flattened'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Hrita\\UC-Irvine\\CS-178\\project\\neuralnet.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Hrita/UC-Irvine/CS-178/project/neuralnet.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dummy_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m100\u001b[39m) \n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Hrita/UC-Irvine/CS-178/project/neuralnet.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dummy_output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49membedded_to_flattened(dummy_input)  \u001b[39m# You might need to implement this method in your model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Hrita/UC-Irvine/CS-178/project/neuralnet.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(dummy_output\u001b[39m.\u001b[39msize())\n",
      "File \u001b[1;32mc:\\Users\\hrita\\anaconda3\\envs\\dirac\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CNN' object has no attribute 'embedded_to_flattened'"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 1, 100, 100) \n",
    "dummy_output = model.embedded_to_flattened(dummy_input)  # You might need to implement this method in your model\n",
    "print(dummy_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n",
      "Training epoch 0, batch 0\n",
      "Training epoch 0, batch 20\n",
      "Training epoch 0, batch 40\n",
      "Training epoch 0, batch 60\n",
      "Training epoch 0, batch 80\n",
      "Training epoch 0, batch 100\n",
      "Training epoch 0, batch 120\n",
      "Training epoch 0, batch 140\n",
      "Training epoch 0, batch 160\n",
      "Training epoch 0, batch 180\n",
      "Training epoch 0, batch 200\n",
      "Training epoch 0, batch 220\n",
      "Training epoch 0, batch 240\n",
      "Training epoch 0, batch 260\n",
      "Training epoch 0, batch 280\n",
      "Training epoch 0, batch 300\n",
      "Training epoch 0, batch 320\n",
      "Training epoch 0, batch 340\n",
      "Training epoch 0, batch 360\n",
      "Training epoch 0, batch 380\n",
      "Training epoch 0, batch 400\n",
      "Training epoch 0, batch 420\n",
      "Training epoch 0, batch 440\n",
      "Training epoch 0, batch 460\n",
      "Training epoch 0, batch 480\n",
      "Training epoch 0, batch 500\n",
      "Training epoch 0, batch 520\n",
      "Training epoch 0, batch 540\n",
      "Training epoch 0, batch 560\n",
      "Training epoch 0, batch 580\n",
      "Training epoch 0, batch 600\n",
      "Training epoch 0, batch 620\n",
      "Training epoch 0, batch 640\n",
      "Training epoch 0, batch 660\n",
      "Training epoch 0, batch 680\n",
      "Training epoch 0, batch 700\n",
      "Training epoch 0, batch 720\n",
      "Training epoch 0, batch 740\n",
      "Training epoch 0, batch 760\n",
      "Training epoch 0, batch 780\n",
      "Training epoch 0, batch 800\n",
      "Training epoch 0, batch 820\n",
      "Training epoch 0, batch 840\n",
      "Training epoch 0, batch 860\n",
      "Training epoch 0, batch 880\n",
      "Training epoch 0, batch 900\n",
      "Training epoch 0, batch 920\n",
      "Training epoch 0, batch 940\n",
      "Training epoch 0, batch 960\n",
      "Training epoch 0, batch 980\n",
      "Training epoch 0, batch 1000\n",
      "Training epoch 0, batch 1020\n",
      "Training epoch 0, batch 1040\n",
      "Training epoch 0, batch 1060\n",
      "Training epoch 0, batch 1080\n",
      "Training epoch 0, batch 1100\n",
      "Training epoch 0, batch 1120\n",
      "Training epoch 0, batch 1140\n",
      "Training epoch 0, batch 1160\n",
      "Training epoch 0, batch 1180\n",
      "Training epoch 0, batch 1200\n",
      "Training epoch 0, batch 1220\n",
      "Training epoch 0, batch 1240\n",
      "Training epoch 0, batch 1260\n",
      "Training epoch 0, batch 1280\n",
      "Training epoch 0, batch 1300\n",
      "Training epoch 0, batch 1320\n",
      "Training epoch 0, batch 1340\n",
      "Training epoch 0, batch 1360\n",
      "Training epoch 0, batch 1380\n",
      "Training epoch 0, batch 1400\n",
      "Training epoch 0, batch 1420\n",
      "Training epoch 0, batch 1440\n",
      "Training epoch 0, batch 1460\n",
      "Training epoch 0, batch 1480\n",
      "Training epoch 0, batch 1500\n",
      "Training epoch 0, batch 1520\n",
      "Training epoch 0, batch 1540\n",
      "Training epoch 0, batch 1560\n",
      "Training epoch 0, batch 1580\n",
      "Training epoch 0, batch 1600\n",
      "Training epoch 0, batch 1620\n",
      "Training epoch 0, batch 1640\n",
      "Training epoch 0, batch 1660\n",
      "Training epoch 0, batch 1680\n",
      "Training epoch 0, batch 1700\n",
      "Training epoch 0, batch 1720\n",
      "Training epoch 0, batch 1740\n",
      "Training epoch 0, batch 1760\n",
      "Training epoch 0, batch 1780\n",
      "Training epoch 0, batch 1800\n",
      "Training epoch 0, batch 1820\n",
      "Training epoch 0, batch 1840\n",
      "Training epoch 0, batch 1860\n",
      "Training epoch 0, batch 1880\n",
      "Training epoch 0, batch 1900\n",
      "Training epoch 0, batch 1920\n",
      "Training epoch 0, batch 1940\n",
      "Training epoch 0, batch 1960\n",
      "Training epoch 0, batch 1980\n",
      "Epoch: 1, Loss: 0.08690692154191117\n",
      "Training epoch 1, batch 0\n",
      "Training epoch 1, batch 20\n",
      "Training epoch 1, batch 40\n",
      "Training epoch 1, batch 60\n",
      "Training epoch 1, batch 80\n",
      "Training epoch 1, batch 100\n",
      "Training epoch 1, batch 120\n",
      "Training epoch 1, batch 140\n",
      "Training epoch 1, batch 160\n",
      "Training epoch 1, batch 180\n",
      "Training epoch 1, batch 200\n",
      "Training epoch 1, batch 220\n",
      "Training epoch 1, batch 240\n",
      "Training epoch 1, batch 260\n",
      "Training epoch 1, batch 280\n",
      "Training epoch 1, batch 300\n",
      "Training epoch 1, batch 320\n",
      "Training epoch 1, batch 340\n",
      "Training epoch 1, batch 360\n",
      "Training epoch 1, batch 380\n",
      "Training epoch 1, batch 400\n",
      "Training epoch 1, batch 420\n",
      "Training epoch 1, batch 440\n",
      "Training epoch 1, batch 460\n",
      "Training epoch 1, batch 480\n",
      "Training epoch 1, batch 500\n",
      "Training epoch 1, batch 520\n",
      "Training epoch 1, batch 540\n",
      "Training epoch 1, batch 560\n",
      "Training epoch 1, batch 580\n",
      "Training epoch 1, batch 600\n",
      "Training epoch 1, batch 620\n",
      "Training epoch 1, batch 640\n",
      "Training epoch 1, batch 660\n",
      "Training epoch 1, batch 680\n",
      "Training epoch 1, batch 700\n",
      "Training epoch 1, batch 720\n",
      "Training epoch 1, batch 740\n",
      "Training epoch 1, batch 760\n",
      "Training epoch 1, batch 780\n",
      "Training epoch 1, batch 800\n",
      "Training epoch 1, batch 820\n",
      "Training epoch 1, batch 840\n",
      "Training epoch 1, batch 860\n",
      "Training epoch 1, batch 880\n",
      "Training epoch 1, batch 900\n",
      "Training epoch 1, batch 920\n",
      "Training epoch 1, batch 940\n",
      "Training epoch 1, batch 960\n",
      "Training epoch 1, batch 980\n",
      "Training epoch 1, batch 1000\n",
      "Training epoch 1, batch 1020\n",
      "Training epoch 1, batch 1040\n",
      "Training epoch 1, batch 1060\n",
      "Training epoch 1, batch 1080\n",
      "Training epoch 1, batch 1100\n",
      "Training epoch 1, batch 1120\n",
      "Training epoch 1, batch 1140\n",
      "Training epoch 1, batch 1160\n",
      "Training epoch 1, batch 1180\n",
      "Training epoch 1, batch 1200\n",
      "Training epoch 1, batch 1220\n",
      "Training epoch 1, batch 1240\n",
      "Training epoch 1, batch 1260\n",
      "Training epoch 1, batch 1280\n",
      "Training epoch 1, batch 1300\n",
      "Training epoch 1, batch 1320\n",
      "Training epoch 1, batch 1340\n",
      "Training epoch 1, batch 1360\n",
      "Training epoch 1, batch 1380\n",
      "Training epoch 1, batch 1400\n",
      "Training epoch 1, batch 1420\n",
      "Training epoch 1, batch 1440\n",
      "Training epoch 1, batch 1460\n",
      "Training epoch 1, batch 1480\n",
      "Training epoch 1, batch 1500\n",
      "Training epoch 1, batch 1520\n",
      "Training epoch 1, batch 1540\n",
      "Training epoch 1, batch 1560\n",
      "Training epoch 1, batch 1580\n",
      "Training epoch 1, batch 1600\n",
      "Training epoch 1, batch 1620\n",
      "Training epoch 1, batch 1640\n",
      "Training epoch 1, batch 1660\n",
      "Training epoch 1, batch 1680\n",
      "Training epoch 1, batch 1700\n",
      "Training epoch 1, batch 1720\n",
      "Training epoch 1, batch 1740\n",
      "Training epoch 1, batch 1760\n",
      "Training epoch 1, batch 1780\n",
      "Training epoch 1, batch 1800\n",
      "Training epoch 1, batch 1820\n",
      "Training epoch 1, batch 1840\n",
      "Training epoch 1, batch 1860\n",
      "Training epoch 1, batch 1880\n",
      "Training epoch 1, batch 1900\n",
      "Training epoch 1, batch 1920\n",
      "Training epoch 1, batch 1940\n",
      "Training epoch 1, batch 1960\n",
      "Training epoch 1, batch 1980\n",
      "Epoch: 2, Loss: 0.06064554438307732\n",
      "Training epoch 2, batch 0\n",
      "Training epoch 2, batch 20\n",
      "Training epoch 2, batch 40\n",
      "Training epoch 2, batch 60\n",
      "Training epoch 2, batch 80\n",
      "Training epoch 2, batch 100\n",
      "Training epoch 2, batch 120\n",
      "Training epoch 2, batch 140\n",
      "Training epoch 2, batch 160\n",
      "Training epoch 2, batch 180\n",
      "Training epoch 2, batch 200\n",
      "Training epoch 2, batch 220\n",
      "Training epoch 2, batch 240\n",
      "Training epoch 2, batch 260\n",
      "Training epoch 2, batch 280\n",
      "Training epoch 2, batch 300\n",
      "Training epoch 2, batch 320\n",
      "Training epoch 2, batch 340\n",
      "Training epoch 2, batch 360\n",
      "Training epoch 2, batch 380\n",
      "Training epoch 2, batch 400\n",
      "Training epoch 2, batch 420\n",
      "Training epoch 2, batch 440\n",
      "Training epoch 2, batch 460\n",
      "Training epoch 2, batch 480\n",
      "Training epoch 2, batch 500\n",
      "Training epoch 2, batch 520\n",
      "Training epoch 2, batch 540\n",
      "Training epoch 2, batch 560\n",
      "Training epoch 2, batch 580\n",
      "Training epoch 2, batch 600\n",
      "Training epoch 2, batch 620\n",
      "Training epoch 2, batch 640\n",
      "Training epoch 2, batch 660\n",
      "Training epoch 2, batch 680\n",
      "Training epoch 2, batch 700\n",
      "Training epoch 2, batch 720\n",
      "Training epoch 2, batch 740\n",
      "Training epoch 2, batch 760\n",
      "Training epoch 2, batch 780\n",
      "Training epoch 2, batch 800\n",
      "Training epoch 2, batch 820\n",
      "Training epoch 2, batch 840\n",
      "Training epoch 2, batch 860\n",
      "Training epoch 2, batch 880\n",
      "Training epoch 2, batch 900\n",
      "Training epoch 2, batch 920\n",
      "Training epoch 2, batch 940\n",
      "Training epoch 2, batch 960\n",
      "Training epoch 2, batch 980\n",
      "Training epoch 2, batch 1000\n",
      "Training epoch 2, batch 1020\n",
      "Training epoch 2, batch 1040\n",
      "Training epoch 2, batch 1060\n",
      "Training epoch 2, batch 1080\n",
      "Training epoch 2, batch 1100\n",
      "Training epoch 2, batch 1120\n",
      "Training epoch 2, batch 1140\n",
      "Training epoch 2, batch 1160\n",
      "Training epoch 2, batch 1180\n",
      "Training epoch 2, batch 1200\n",
      "Training epoch 2, batch 1220\n",
      "Training epoch 2, batch 1240\n",
      "Training epoch 2, batch 1260\n",
      "Training epoch 2, batch 1280\n",
      "Training epoch 2, batch 1300\n",
      "Training epoch 2, batch 1320\n",
      "Training epoch 2, batch 1340\n",
      "Training epoch 2, batch 1360\n",
      "Training epoch 2, batch 1380\n",
      "Training epoch 2, batch 1400\n",
      "Training epoch 2, batch 1420\n",
      "Training epoch 2, batch 1440\n",
      "Training epoch 2, batch 1460\n",
      "Training epoch 2, batch 1480\n",
      "Training epoch 2, batch 1500\n",
      "Training epoch 2, batch 1520\n",
      "Training epoch 2, batch 1540\n",
      "Training epoch 2, batch 1560\n",
      "Training epoch 2, batch 1580\n",
      "Training epoch 2, batch 1600\n",
      "Training epoch 2, batch 1620\n",
      "Training epoch 2, batch 1640\n",
      "Training epoch 2, batch 1660\n",
      "Training epoch 2, batch 1680\n",
      "Training epoch 2, batch 1700\n",
      "Training epoch 2, batch 1720\n",
      "Training epoch 2, batch 1740\n",
      "Training epoch 2, batch 1760\n",
      "Training epoch 2, batch 1780\n",
      "Training epoch 2, batch 1800\n",
      "Training epoch 2, batch 1820\n",
      "Training epoch 2, batch 1840\n",
      "Training epoch 2, batch 1860\n",
      "Training epoch 2, batch 1880\n",
      "Training epoch 2, batch 1900\n",
      "Training epoch 2, batch 1920\n",
      "Training epoch 2, batch 1940\n",
      "Training epoch 2, batch 1960\n",
      "Training epoch 2, batch 1980\n",
      "Epoch: 3, Loss: 0.051724777645069664\n",
      "Training epoch 3, batch 0\n",
      "Training epoch 3, batch 20\n",
      "Training epoch 3, batch 40\n",
      "Training epoch 3, batch 60\n",
      "Training epoch 3, batch 80\n",
      "Training epoch 3, batch 100\n",
      "Training epoch 3, batch 120\n",
      "Training epoch 3, batch 140\n",
      "Training epoch 3, batch 160\n",
      "Training epoch 3, batch 180\n",
      "Training epoch 3, batch 200\n",
      "Training epoch 3, batch 220\n",
      "Training epoch 3, batch 240\n",
      "Training epoch 3, batch 260\n",
      "Training epoch 3, batch 280\n",
      "Training epoch 3, batch 300\n",
      "Training epoch 3, batch 320\n",
      "Training epoch 3, batch 340\n",
      "Training epoch 3, batch 360\n",
      "Training epoch 3, batch 380\n",
      "Training epoch 3, batch 400\n",
      "Training epoch 3, batch 420\n",
      "Training epoch 3, batch 440\n",
      "Training epoch 3, batch 460\n",
      "Training epoch 3, batch 480\n",
      "Training epoch 3, batch 500\n",
      "Training epoch 3, batch 520\n",
      "Training epoch 3, batch 540\n",
      "Training epoch 3, batch 560\n",
      "Training epoch 3, batch 580\n",
      "Training epoch 3, batch 600\n",
      "Training epoch 3, batch 620\n",
      "Training epoch 3, batch 640\n",
      "Training epoch 3, batch 660\n",
      "Training epoch 3, batch 680\n",
      "Training epoch 3, batch 700\n",
      "Training epoch 3, batch 720\n",
      "Training epoch 3, batch 740\n",
      "Training epoch 3, batch 760\n",
      "Training epoch 3, batch 780\n",
      "Training epoch 3, batch 800\n",
      "Training epoch 3, batch 820\n",
      "Training epoch 3, batch 840\n",
      "Training epoch 3, batch 860\n",
      "Training epoch 3, batch 880\n",
      "Training epoch 3, batch 900\n",
      "Training epoch 3, batch 920\n",
      "Training epoch 3, batch 940\n",
      "Training epoch 3, batch 960\n",
      "Training epoch 3, batch 980\n",
      "Training epoch 3, batch 1000\n",
      "Training epoch 3, batch 1020\n",
      "Training epoch 3, batch 1040\n",
      "Training epoch 3, batch 1060\n",
      "Training epoch 3, batch 1080\n",
      "Training epoch 3, batch 1100\n",
      "Training epoch 3, batch 1120\n",
      "Training epoch 3, batch 1140\n",
      "Training epoch 3, batch 1160\n",
      "Training epoch 3, batch 1180\n",
      "Training epoch 3, batch 1200\n",
      "Training epoch 3, batch 1220\n",
      "Training epoch 3, batch 1240\n",
      "Training epoch 3, batch 1260\n",
      "Training epoch 3, batch 1280\n",
      "Training epoch 3, batch 1300\n",
      "Training epoch 3, batch 1320\n",
      "Training epoch 3, batch 1340\n",
      "Training epoch 3, batch 1360\n",
      "Training epoch 3, batch 1380\n",
      "Training epoch 3, batch 1400\n",
      "Training epoch 3, batch 1420\n",
      "Training epoch 3, batch 1440\n",
      "Training epoch 3, batch 1460\n",
      "Training epoch 3, batch 1480\n",
      "Training epoch 3, batch 1500\n",
      "Training epoch 3, batch 1520\n",
      "Training epoch 3, batch 1540\n",
      "Training epoch 3, batch 1560\n",
      "Training epoch 3, batch 1580\n",
      "Training epoch 3, batch 1600\n",
      "Training epoch 3, batch 1620\n",
      "Training epoch 3, batch 1640\n",
      "Training epoch 3, batch 1660\n",
      "Training epoch 3, batch 1680\n",
      "Training epoch 3, batch 1700\n",
      "Training epoch 3, batch 1720\n",
      "Training epoch 3, batch 1740\n",
      "Training epoch 3, batch 1760\n",
      "Training epoch 3, batch 1780\n",
      "Training epoch 3, batch 1800\n",
      "Training epoch 3, batch 1820\n",
      "Training epoch 3, batch 1840\n",
      "Training epoch 3, batch 1860\n",
      "Training epoch 3, batch 1880\n",
      "Training epoch 3, batch 1900\n",
      "Training epoch 3, batch 1920\n",
      "Training epoch 3, batch 1940\n",
      "Training epoch 3, batch 1960\n",
      "Training epoch 3, batch 1980\n",
      "Epoch: 4, Loss: 0.04653712265977734\n",
      "Training epoch 4, batch 0\n",
      "Training epoch 4, batch 20\n",
      "Training epoch 4, batch 40\n",
      "Training epoch 4, batch 60\n",
      "Training epoch 4, batch 80\n",
      "Training epoch 4, batch 100\n",
      "Training epoch 4, batch 120\n",
      "Training epoch 4, batch 140\n",
      "Training epoch 4, batch 160\n",
      "Training epoch 4, batch 180\n",
      "Training epoch 4, batch 200\n",
      "Training epoch 4, batch 220\n",
      "Training epoch 4, batch 240\n",
      "Training epoch 4, batch 260\n",
      "Training epoch 4, batch 280\n",
      "Training epoch 4, batch 300\n",
      "Training epoch 4, batch 320\n",
      "Training epoch 4, batch 340\n",
      "Training epoch 4, batch 360\n",
      "Training epoch 4, batch 380\n",
      "Training epoch 4, batch 400\n",
      "Training epoch 4, batch 420\n",
      "Training epoch 4, batch 440\n",
      "Training epoch 4, batch 460\n",
      "Training epoch 4, batch 480\n",
      "Training epoch 4, batch 500\n",
      "Training epoch 4, batch 520\n",
      "Training epoch 4, batch 540\n",
      "Training epoch 4, batch 560\n",
      "Training epoch 4, batch 580\n",
      "Training epoch 4, batch 600\n",
      "Training epoch 4, batch 620\n",
      "Training epoch 4, batch 640\n",
      "Training epoch 4, batch 660\n",
      "Training epoch 4, batch 680\n",
      "Training epoch 4, batch 700\n",
      "Training epoch 4, batch 720\n",
      "Training epoch 4, batch 740\n",
      "Training epoch 4, batch 760\n",
      "Training epoch 4, batch 780\n",
      "Training epoch 4, batch 800\n",
      "Training epoch 4, batch 820\n",
      "Training epoch 4, batch 840\n",
      "Training epoch 4, batch 860\n",
      "Training epoch 4, batch 880\n",
      "Training epoch 4, batch 900\n",
      "Training epoch 4, batch 920\n",
      "Training epoch 4, batch 940\n",
      "Training epoch 4, batch 960\n",
      "Training epoch 4, batch 980\n",
      "Training epoch 4, batch 1000\n",
      "Training epoch 4, batch 1020\n",
      "Training epoch 4, batch 1040\n",
      "Training epoch 4, batch 1060\n",
      "Training epoch 4, batch 1080\n",
      "Training epoch 4, batch 1100\n",
      "Training epoch 4, batch 1120\n",
      "Training epoch 4, batch 1140\n",
      "Training epoch 4, batch 1160\n",
      "Training epoch 4, batch 1180\n",
      "Training epoch 4, batch 1200\n",
      "Training epoch 4, batch 1220\n",
      "Training epoch 4, batch 1240\n",
      "Training epoch 4, batch 1260\n",
      "Training epoch 4, batch 1280\n",
      "Training epoch 4, batch 1300\n",
      "Training epoch 4, batch 1320\n",
      "Training epoch 4, batch 1340\n",
      "Training epoch 4, batch 1360\n",
      "Training epoch 4, batch 1380\n",
      "Training epoch 4, batch 1400\n",
      "Training epoch 4, batch 1420\n",
      "Training epoch 4, batch 1440\n",
      "Training epoch 4, batch 1460\n",
      "Training epoch 4, batch 1480\n",
      "Training epoch 4, batch 1500\n",
      "Training epoch 4, batch 1520\n",
      "Training epoch 4, batch 1540\n",
      "Training epoch 4, batch 1560\n",
      "Training epoch 4, batch 1580\n",
      "Training epoch 4, batch 1600\n",
      "Training epoch 4, batch 1620\n",
      "Training epoch 4, batch 1640\n",
      "Training epoch 4, batch 1660\n",
      "Training epoch 4, batch 1680\n",
      "Training epoch 4, batch 1700\n",
      "Training epoch 4, batch 1720\n",
      "Training epoch 4, batch 1740\n",
      "Training epoch 4, batch 1760\n",
      "Training epoch 4, batch 1780\n",
      "Training epoch 4, batch 1800\n",
      "Training epoch 4, batch 1820\n",
      "Training epoch 4, batch 1840\n",
      "Training epoch 4, batch 1860\n",
      "Training epoch 4, batch 1880\n",
      "Training epoch 4, batch 1900\n",
      "Training epoch 4, batch 1920\n",
      "Training epoch 4, batch 1940\n",
      "Training epoch 4, batch 1960\n",
      "Training epoch 4, batch 1980\n",
      "Epoch: 5, Loss: 0.04203620775588753\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress all UserWarnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Move model and criterion to GPU (if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device: ', device)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    counter = 0\n",
    "    for texts, labels, _ in train_loader:\n",
    "        if (counter%20) == 0:\n",
    "            print(f'Training epoch {epoch}, batch {counter}')\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(texts).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        counter+=1\n",
    "    print(f'Epoch: {epoch+1}, Loss: {epoch_loss/len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.054111703961550114\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for texts, labels, _ in test_loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        predictions = model(texts).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        test_loss += loss.item()\n",
    "print(f'Test Loss: {test_loss/len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try our own texts\n",
    "\n",
    "def preprocess_text(text, tokenizer, vocab, max_length):\n",
    "    tokens = tokenizer(text)\n",
    "    token_ids = [vocab[token] for token in tokens]  # Use vocab directly\n",
    "    \n",
    "    # Pad or truncate the sequence to a fixed length\n",
    "    if len(token_ids) < max_length:\n",
    "        token_ids += [vocab['<pad>']] * (max_length - len(token_ids))\n",
    "    else:\n",
    "        token_ids = token_ids[:max_length]\n",
    "\n",
    "    return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "# Example usage\n",
    "text_good = \"good job\"\n",
    "text_bad = \"brotha, fuk you\"\n",
    "max_length = 100  # or whatever length your model expects\n",
    "processed_text = preprocess_text(text_bad, tokenizer, vocab, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'toxic': True, 'severe_toxic': True, 'obscene': True, 'threat': False, 'insult': True, 'identity_hate': False}\n"
     ]
    }
   ],
   "source": [
    "# Switch model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move data to the appropriate device\n",
    "processed_text = processed_text.unsqueeze(0)  # Add batch dimension\n",
    "processed_text = processed_text.to(device)  # Assuming 'device' is defined\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(processed_text)\n",
    "    predictions = torch.sigmoid(predictions)\n",
    "    predicted_labels = (predictions > 0.5).int()\n",
    "\n",
    "# Convert predicted labels to a readable format\n",
    "predicted_labels = predicted_labels.squeeze(0).cpu().numpy()\n",
    "label_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "results = {label: bool(pred) for label, pred in zip(label_names, predicted_labels)}\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dirac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
